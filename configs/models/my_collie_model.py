from opencompass.models import HuggingFaceCausalLM

models = [
    #
    # dict(
    #     abbr="llama-7b-adamw",
    #     type=HuggingFaceCausalLM,
    #     path="/mnt/petrelfs/chenkeyu1/models/collie/llama-7b/adamw/",
    #     tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           use_fast=False,
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     model_kwargs=dict(device_map='auto'),
    #     batch_padding=False,  # if false, inference with for-loop without batch padding
    #     run_cfg=dict(num_gpus=1, num_procs=1),
    # ),
    # dict(
    #     abbr="llama-7b-adan",
    #     type=HuggingFaceCausalLM,
    #     path="/mnt/petrelfs/chenkeyu1/models/collie/llama-7b/adan_lr-1e-4_weight-decay-2e-2_epoch-3/",
    #     tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           use_fast=False,
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     model_kwargs=dict(device_map='auto'),
    #     batch_padding=False,  # if false, inference with for-loop without batch padding
    #     run_cfg=dict(num_gpus=1, num_procs=1),
    # ),
    # dict(
    #     abbr="llama-7b-sophia",
    #     type=HuggingFaceCausalLM,
    #     path="/mnt/petrelfs/chenkeyu1/models/collie/llama-7b/sophia/",
    #     tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           use_fast=False,
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     model_kwargs=dict(device_map='auto'),
    #     batch_padding=False,  # if false, inference with for-loop without batch padding
    #     run_cfg=dict(num_gpus=1, num_procs=1),
    # ),
    # dict(
    #     type=HuggingFaceCausalLM,
    #     abbr='llama-7b-lora',
    #     path="/mnt/petrelfs/chenkeyu1/models/llama/llama-7b-hf",
    #     tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #     peft_path="/mnt/petrelfs/chenkeyu1/models/collie/llama-7b/lora",
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           use_fast=False,
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=8,
    #     model_kwargs=dict(device_map='auto'),
    #     batch_padding=False,  # if false, inference with for-loop without batch padding
    #     run_cfg=dict(num_gpus=1, num_procs=1),
    # ),
    #
    # dict(
    #     abbr="lomo_lr-0.01_grad-norm-1.0_epoch-3_zero0",
    #     type=HuggingFaceCausalLM,
    #     path="/mnt/petrelfs/chenkeyu1/models/collie/llama-7b/lomo_lr-0.01_grad-norm-1.0_epoch-3_zero0/",
    #     tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           use_fast=False,
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     model_kwargs=dict(device_map='auto'),
    #     batch_padding=False,  # if false, inference with for-loop without batch padding
    #     run_cfg=dict(num_gpus=1, num_procs=1),
    # ),
    #
    # dict(
    #     abbr="lomo_lr-0.01_grad-norm-1.0_weight-decay-0.0_epoch-3_tp8_dp1_bs64",
    #     type=HuggingFaceCausalLM,
    #     path="/mnt/petrelfs/chenkeyu1/models/collie/llama-7b/lomo_lr-0.01_grad-norm-1.0_weight-decay-0.0_epoch-3_tp8_dp1_bs64/",
    #     tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           use_fast=False,
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=16,
    #     model_kwargs=dict(device_map='auto'),
    #     batch_padding=False,  # if false, inference with for-loop without batch padding
    #     run_cfg=dict(num_gpus=1, num_procs=1),
    # ),
    #
    # dict(
    #     type=HuggingFaceCausalLM,
    #     abbr='llama-7b',
    #     path="/mnt/petrelfs/chenkeyu1/models/llama/llama-7b-hf",
    #     tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           use_fast=False,
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=8,
    #     model_kwargs=dict(device_map='auto'),
    #     batch_padding=False,  # if false, inference with for-loop without batch padding
    #     run_cfg=dict(num_gpus=1, num_procs=1),
    # ),

    # dict(
    # abbr="lomo_lr-0.01_grad-norm-1.0_epoch-3_zero0",
    #         type=HuggingFaceCausalLM,
    #         path="/mnt/petrelfs/chenkeyu1/models/collie/llama-7b/lomo_lr-0.01_grad-norm-1.0_epoch-3_zero0/",
    #         tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #         tokenizer_kwargs=dict(padding_side='left',
    #                               truncation_side='left',
    #                               use_fast=False,
    #                               ),
    #         max_out_len=100,
    #         max_seq_len=2048,
    #         batch_size=8,
    #         model_kwargs=dict(device_map='auto'),
    #         batch_padding=False, # if false, inference with for-loop without batch padding
    #         run_cfg=dict(num_gpus=1, num_procs=1),
    #     ),
    # dict(
    # abbr="lomo_lr-0.01_grad-norm-5.0_epoch-3",
    #         type=HuggingFaceCausalLM,
    #         path="/mnt/petrelfs/chenkeyu1/models/collie/llama-7b/lomo_lr-0.01_grad-norm-5.0_epoch-3/",
    #         tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #         tokenizer_kwargs=dict(padding_side='left',
    #                               truncation_side='left',
    #                               use_fast=False,
    #                               ),
    #         max_out_len=100,
    #         max_seq_len=2048,
    #         batch_size=8,
    #         model_kwargs=dict(device_map='auto'),
    #         batch_padding=False, # if false, inference with for-loop without batch padding
    #         run_cfg=dict(num_gpus=1, num_procs=1),
    #     ),
    #

    #     dict(
    #         type=HuggingFaceCausalLM,
    #         # abbr='llama-2-7b-hf',
    #         # path="meta-llama/Llama-2-7b-hf",
    #         # tokenizer_path='meta-llama/Llama-2-7b-hf',
    #         path="/mnt/petrelfs/chenkeyu1/models/collie/llama-30b/adamw/",
    #         tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #         tokenizer_kwargs=dict(padding_side='left',
    #                               truncation_side='left',
    #                               use_fast=False,
    #                               ),
    #         max_out_len=100,
    #         max_seq_len=2048,
    #         batch_size=8,
    #         model_kwargs=dict(device_map='auto'),
    #         batch_padding=False, # if false, inference with for-loop without batch padding
    #         run_cfg=dict(num_gpus=4, num_procs=1),
    #     ),

    # dict(
    #         type=HuggingFaceCausalLM,
    #         path="/mnt/petrelfs/chenkeyu1/models/collie/llama-30b/adan/",
    #         tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #         tokenizer_kwargs=dict(padding_side='left',
    #                               truncation_side='left',
    #                               use_fast=False,
    #                               ),
    #         max_out_len=100,
    #         max_seq_len=8192,
    #         batch_size=8,
    #         model_kwargs=dict(device_map='auto'),
    #         batch_padding=True, # if false, inference with for-loop without batch padding
    #         run_cfg=dict(num_gpus=4, num_procs=1),
    #     ),
    # dict(
    #         type=HuggingFaceCausalLM,
    #         path="/mnt/petrelfs/chenkeyu1/models/collie/llama-30b/lion/",
    #         tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #         tokenizer_kwargs=dict(padding_side='left',
    #                               truncation_side='left',
    #                               use_fast=False,
    #                               ),
    #         max_out_len=100,
    #         max_seq_len=2048,
    #         batch_size=8,
    #         model_kwargs=dict(device_map='auto'),
    #         batch_padding=False, # if false, inference with for-loop without batch padding
    #         run_cfg=dict(num_gpus=4, num_procs=1),
    #     ),
    # dict(
    #         type=HuggingFaceCausalLM,
    #         path="/mnt/petrelfs/chenkeyu1/models/collie/llama-30b/sophia/",
    #         tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #         tokenizer_kwargs=dict(padding_side='left',
    #                               truncation_side='left',
    #                               use_fast=False,
    #                               ),
    #         max_out_len=100,
    #         max_seq_len=2048,
    #         batch_size=8,
    #         model_kwargs=dict(device_map='auto'),
    #         batch_padding=False, # if false, inference with for-loop without batch padding
    #         run_cfg=dict(num_gpus=4, num_procs=1),
    #     ),

    # dict(
    #         type=HuggingFaceCausalLM,
    #         path="/mnt/petrelfs/chenkeyu1/models/collie/llama-30b/lomo/",
    #         tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #         tokenizer_kwargs=dict(padding_side='left',
    #                               truncation_side='left',
    #                               use_fast=False,
    #                               ),
    #         max_out_len=100,
    #         max_seq_len=2048,
    #         batch_size=8,
    #         model_kwargs=dict(device_map='auto'),
    #         batch_padding=False, # if false, inference with for-loop without batch padding
    #         run_cfg=dict(num_gpus=4, num_procs=1),
    #     ),

    # dict(
    #     type=HuggingFaceCausalLM,
    #     abbr='llama-30b-lora',
    #     # path="meta-llama/Llama-2-7b-hf",
    #     # tokenizer_path='meta-llama/Llama-2-7b-hf',
    #     path="/mnt/petrelfs/chenkeyu1/models/llama/llama-30b-hf",
    #     tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #     peft_path="/mnt/petrelfs/chenkeyu1/models/collie/llama-30b/lora",
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           use_fast=False,
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=8,
    #     model_kwargs=dict(device_map='auto'),
    #     batch_padding=False,  # if false, inference with for-loop without batch padding
    #     run_cfg=dict(num_gpus=4, num_procs=1),
    # ),

    # dict(
    #     type=HuggingFaceCausalLM,
    #     abbr='llama-13b-lora',
    #     path="/mnt/petrelfs/chenkeyu1/models/llama/llama-13b-hf",
    #     tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #     peft_path="/mnt/petrelfs/chenkeyu1/models/collie/llama-13b/lora_lr-0.0003_epoch-3",
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           use_fast=False,
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=8,
    #     model_kwargs=dict(device_map='auto'),
    #     batch_padding=False,  # if false, inference with for-loop without batch padding
    #     run_cfg=dict(num_gpus=2, num_procs=1),
    # ),
    #
    # dict(
    # abbr="lomo_lr-0.01_grad-norm-1.0_epoch-3",
    #         type=HuggingFaceCausalLM,
    #         path="/mnt/petrelfs/chenkeyu1/models/collie/llama-13b/lomo_lr-0.01_grad-norm-1.0_epoch-3",
    #         tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #         tokenizer_kwargs=dict(padding_side='left',
    #                               truncation_side='left',
    #                               use_fast=False,
    #                               ),
    #         max_out_len=100,
    #         max_seq_len=2048,
    #         batch_size=8,
    #         model_kwargs=dict(device_map='auto'),
    #         batch_padding=False, # if false, inference with for-loop without batch padding
    #         run_cfg=dict(num_gpus=2, num_procs=1),
    #     ),

    # dict(
    # abbr="lomo_lr-0.003_grad-norm-1.0_epoch-3",
    #         type=HuggingFaceCausalLM,
    #         path="/mnt/petrelfs/chenkeyu1/models/collie/llama-13b/lomo_lr-0.003_grad-norm-1.0_epoch-3/",
    #         tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #         tokenizer_kwargs=dict(padding_side='left',
    #                               truncation_side='left',
    #                               use_fast=False,
    #                               ),
    #         max_out_len=100,
    #         max_seq_len=2048,
    #         batch_size=8,
    #         model_kwargs=dict(device_map='auto'),
    #         batch_padding=False, # if false, inference with for-loop without batch padding
    #         run_cfg=dict(num_gpus=2, num_procs=1),
    #     ),

    # dict(
    # abbr="adan_lr-5e-05_weight-decay-0.02_epoch-3",
    #         type=HuggingFaceCausalLM,
    #         path="/mnt/petrelfs/chenkeyu1/models/collie/llama-65b/adan_lr-5e-05_weight-decay-0.02_epoch-3/",
    #         tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #         tokenizer_kwargs=dict(padding_side='left',
    #                               truncation_side='left',
    #                               use_fast=False,
    #                               ),
    #         max_out_len=100,
    #         max_seq_len=2048,
    #         batch_size=16,
    #         model_kwargs=dict(device_map='auto'),
    #         batch_padding=False, # if false, inference with for-loop without batch padding
    #         run_cfg=dict(num_gpus=8, num_procs=1),
    #     ),
    #
    #
    # dict(
    # abbr="65B-lion_lr-3e-06_weight-decay-0.03_epoch-3",
    #         type=HuggingFaceCausalLM,
    #         path="/mnt/petrelfs/chenkeyu1/models/collie/llama-65b/lion_lr-3e-06_weight-decay-0.03_epoch-3/",
    #         tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #         tokenizer_kwargs=dict(padding_side='left',
    #                               truncation_side='left',
    #                               use_fast=False,
    #                               ),
    #         max_out_len=100,
    #         max_seq_len=2048,
    #         batch_size=8,
    #         model_kwargs=dict(device_map='auto'),
    #         batch_padding=False, # if false, inference with for-loop without batch padding
    #         run_cfg=dict(num_gpus=8, num_procs=1),
    #     ),

    # dict(abbr="LLama65B",
    #      type='LLama', path='/mnt/petrelfs/share_data/llm_llama/65B',
    #      tokenizer_path='/mnt/petrelfs/share_data/llm_llama/tokenizer.model', tokenizer_type='llama',
    #      max_out_len=100, max_seq_len=2048, batch_size=16, run_cfg=dict(num_gpus=8)),
    # dict(
    #     type=HuggingFaceCausalLM,
    #     abbr='llama-65b-lora',
    #     path="/mnt/petrelfs/chenkeyu1/models/llama/llama-65b-hf",
    #     tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
    #     peft_path="/mnt/petrelfs/chenkeyu1/models/collie/llama-65b/lora_lr-0.0003_epoch-3",
    #     tokenizer_kwargs=dict(padding_side='left',
    #                           truncation_side='left',
    #                           use_fast=False,
    #                           ),
    #     max_out_len=100,
    #     max_seq_len=2048,
    #     batch_size=8,
    #     model_kwargs=dict(device_map='auto'),
    #     batch_padding=False,  # if false, inference with for-loop without batch padding
    #     run_cfg=dict(num_gpus=8, num_procs=1),
    # ),

    dict(
        abbr="lion_lr-3e-06_weight-decay-0.03_epoch-3",
        type=HuggingFaceCausalLM,
        path="/mnt/petrelfs/chenkeyu1/models/collie/llama-65b/lion_lr-3e-06_weight-decay-0.03_epoch-3/",
        tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
        tokenizer_kwargs=dict(padding_side='left',
                              truncation_side='left',
                              use_fast=False,
                              ),
        max_out_len=100,
        max_seq_len=2048,
        batch_size=16,
        model_kwargs=dict(device_map='auto'),
        batch_padding=False,  # if false, inference with for-loop without batch padding
        run_cfg=dict(num_gpus=8, num_procs=1),
    ),

    dict(
        abbr="lomo_lr-0.01_grad-norm-1.0_epoch-5_tp8_bs16",
        type=HuggingFaceCausalLM,
        path="/mnt/petrelfs/chenkeyu1/models/collie/llama-65b/lomo_lr-0.01_grad-norm-1.0_epoch-5_tp8_bs16/",
        tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
        tokenizer_kwargs=dict(padding_side='left',
                              truncation_side='left',
                              use_fast=False,
                              ),
        max_out_len=100,
        max_seq_len=2048,
        batch_size=16,
        model_kwargs=dict(device_map='auto'),
        batch_padding=False,  # if false, inference with for-loop without batch padding
        run_cfg=dict(num_gpus=8, num_procs=1),
    ),
    dict(
        abbr="adamw_lr-1e-05_epoch-3",
        type=HuggingFaceCausalLM,
        path="/mnt/petrelfs/chenkeyu1/models/collie/llama-65b/adamw_lr-1e-05_epoch-3/",
        tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
        tokenizer_kwargs=dict(padding_side='left',
                              truncation_side='left',
                              use_fast=False,
                              ),
        max_out_len=100,
        max_seq_len=2048,
        batch_size=16,
        model_kwargs=dict(device_map='auto'),
        batch_padding=False,  # if false, inference with for-loop without batch padding
        run_cfg=dict(num_gpus=8, num_procs=1),
    ),
    dict(
        abbr="adan_lr-5e-05_weight-decay-0.02_epoch-3",
        type=HuggingFaceCausalLM,
        path="/mnt/petrelfs/chenkeyu1/models/collie/llama-65b/adan_lr-5e-05_weight-decay-0.02_epoch-3",
        tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
        tokenizer_kwargs=dict(padding_side='left',
                              truncation_side='left',
                              use_fast=False,
                              ),
        max_out_len=100,
        max_seq_len=2048,
        batch_size=16,
        model_kwargs=dict(device_map='auto'),
        batch_padding=False,  # if false, inference with for-loop without batch padding
        run_cfg=dict(num_gpus=8, num_procs=1),
    ),

#     dict(
#         type=HuggingFaceCausalLM,
#         path="/mnt/petrelfs/chenkeyu1/models/llama/llama-65b-hf",
#         tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
#         tokenizer_kwargs=dict(padding_side='left',
#                               truncation_side='left',
#                               use_fast=False,
#                               ),
#         max_out_len=100,
#         max_seq_len=2048,
#         batch_size=8,
#         model_kwargs=dict(device_map='auto'),
#         batch_padding=False,  # if false, inference with for-loop without batch padding
#         run_cfg=dict(num_gpus=8, num_procs=1),
#     ),
# dict(
#         type=HuggingFaceCausalLM,
#         abbr='llama-65b',
#         path="/mnt/petrelfs/chenkeyu1/models/llama/llama-65b-hf",
#         tokenizer_path='/mnt/petrelfs/chenkeyu1/models/collie/tokenizer/',
#         tokenizer_kwargs=dict(padding_side='left',
#                               truncation_side='left',
#                               use_fast=False,
#                               ),
#         max_out_len=100,
#         max_seq_len=2048,
#         batch_size=8,
#         model_kwargs=dict(device_map='auto'),
#         batch_padding=False,  # if false, inference with for-loop without batch padding
#         run_cfg=dict(num_gpus=8, num_procs=1),
#     ),
]
